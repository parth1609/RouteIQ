{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully using HF_TOKEN environment variable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Logged in successfully using HF_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m      3\u001b[39m api = HfApi(token=\u001b[33m\"\u001b[39m\u001b[33mhf_QofxUJOnVQELsqESXTQlMIrKgXlVEiYWrJ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./gpt2-it-ticket-classifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparth1609/gpt2-it-ticket-classifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1662\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1659\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1661\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1662\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4954\u001b[39m, in \u001b[36mHfApi.upload_folder\u001b[39m\u001b[34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[39m\n\u001b[32m   4944\u001b[39m ignore_patterns += DEFAULT_IGNORE_PATTERNS\n\u001b[32m   4946\u001b[39m delete_operations = \u001b[38;5;28mself\u001b[39m._prepare_folder_deletions(\n\u001b[32m   4947\u001b[39m     repo_id=repo_id,\n\u001b[32m   4948\u001b[39m     repo_type=repo_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4952\u001b[39m     delete_patterns=delete_patterns,\n\u001b[32m   4953\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m4954\u001b[39m add_operations = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_upload_folder_additions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4961\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4963\u001b[39m \u001b[38;5;66;03m# Optimize operations: if some files will be overwritten, we don't need to delete them first\u001b[39;00m\n\u001b[32m   4964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(add_operations) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:9650\u001b[39m, in \u001b[36mHfApi._prepare_upload_folder_additions\u001b[39m\u001b[34m(self, folder_path, path_in_repo, allow_patterns, ignore_patterns, repo_type, token)\u001b[39m\n\u001b[32m   9642\u001b[39m     log(\n\u001b[32m   9643\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt seems you are trying to upload a large folder at once. This might take some time and then fail if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9644\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe folder is too large. For such cases, it is recommended to upload in smaller batches or to use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9645\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`HfApi().upload_large_folder(...)`/`huggingface-cli upload-large-folder` instead. For more details, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9646\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcheck out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9647\u001b[39m     )\n\u001b[32m   9649\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStart hashing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_repo_objects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9650\u001b[39m operations = \u001b[43m[\u001b[49m\n\u001b[32m   9651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCommitOperationAdd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelpath_to_abspath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# absolute path on disk\u001b[39;49;00m\n\u001b[32m   9653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# \"absolute\" path in repo\u001b[39;49;00m\n\u001b[32m   9654\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9655\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiltered_repo_objects\u001b[49m\n\u001b[32m   9656\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   9657\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished hashing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_repo_objects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m operations\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:9651\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   9642\u001b[39m     log(\n\u001b[32m   9643\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt seems you are trying to upload a large folder at once. This might take some time and then fail if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9644\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe folder is too large. For such cases, it is recommended to upload in smaller batches or to use \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9645\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`HfApi().upload_large_folder(...)`/`huggingface-cli upload-large-folder` instead. For more details, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9646\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcheck out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   9647\u001b[39m     )\n\u001b[32m   9649\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStart hashing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_repo_objects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9650\u001b[39m operations = [\n\u001b[32m-> \u001b[39m\u001b[32m9651\u001b[39m     \u001b[43mCommitOperationAdd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelpath_to_abspath\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# absolute path on disk\u001b[39;49;00m\n\u001b[32m   9653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# \"absolute\" path in repo\u001b[39;49;00m\n\u001b[32m   9654\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9655\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m relpath \u001b[38;5;129;01min\u001b[39;00m filtered_repo_objects\n\u001b[32m   9656\u001b[39m ]\n\u001b[32m   9657\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished hashing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_repo_objects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m operations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:5\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, path_in_repo, path_or_fileobj)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\_commit_api.py:201\u001b[39m, in \u001b[36mCommitOperationAdd.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Compute \"upload_info\" attribute\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.path_or_fileobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28mself\u001b[39m.upload_info = \u001b[43mUploadInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.path_or_fileobj, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m.upload_info = UploadInfo.from_bytes(\u001b[38;5;28mself\u001b[39m.path_or_fileobj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\lfs.py:84\u001b[39m, in \u001b[36mUploadInfo.from_path\u001b[39m\u001b[34m(cls, path)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m io.open(path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     83\u001b[39m     sample = file.peek(\u001b[32m512\u001b[39m)[:\u001b[32m512\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     sha = \u001b[43msha_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(size=size, sha256=sha, sample=sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\sha.py:25\u001b[39m, in \u001b[36msha_fileobj\u001b[39m\u001b[34m(fileobj, chunk_size)\u001b[39m\n\u001b[32m     23\u001b[39m sha = sha256()\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     chunk = fileobj.read(chunk_size)\n\u001b[32m     26\u001b[39m     sha.update(chunk)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=hf_token)\n",
    "api.upload_folder(\n",
    "    folder_path=\"./gpt2-it-ticket-classifier\",\n",
    "    repo_id=\"parth1609/gpt2-it-ticket-classifier\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your model ID on Hugging Face Hub\n",
    "MODEL_ID = \"parth1609/gpt2-it-ticket-classifier\"\n",
    "\n",
    "# Your Hugging Face API token\n",
    "# It's highly recommended to load this from an environment variable for security\n",
    "# Make sure you've set HF_TOKEN=\"hf_YOUR_TOKEN_HERE\" in your environment\n",
    "# API_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "API_TOKEN = \"\"\n",
    "\n",
    "if not API_TOKEN:\n",
    "    raise ValueError(\"Hugging Face API token not found. Please set the HF_TOKEN environment variable.\")\n",
    "\n",
    "# The Inference API URL for your model\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\" # Specify content type for JSON payload\n",
    "}\n",
    "\n",
    "# --- Function to send a query to the API ---\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Sends a query (input text) to the Hugging Face Inference API.\n",
    "\n",
    "    Args:\n",
    "        payload (dict): A dictionary containing the input data for the model.\n",
    "                        For text models, it's typically {\"inputs\": \"Your input text\"}.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from the API, containing the model's predictions.\n",
    "    \"\"\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = [\n",
    "    \"My laptop is not turning on after the latest update.\",\n",
    "    \"I need help resetting my VPN password, it keeps failing.\",\n",
    "    \"The software update caused my computer to crash.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\parth\\OneDrive\\Desktop\\one\\ticketingmodel\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2LMHeadModel' is not supported for text2text-generation. Supported models are ['PeftModelForSeq2SeqLM', 'BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Ticket:\n",
      "My laptop is not turning on after the latest update.\n",
      "✅ Model Output:\n",
      "Classify the following IT ticket and return output in JSON format with keys:\n",
      "'ticket', 'Department', 'Priority'.\n",
      "\n",
      "Ticket: \"My laptop is not turning on after the latest update.\"   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\":\n",
      "------------------------------------------------------------\n",
      "🔹 Ticket:\n",
      "The VPN disconnects every time I upload files.\n",
      "✅ Model Output:\n",
      "Classify the following IT ticket and return output in JSON format with keys:\n",
      "'ticket', 'Department', 'Priority'.\n",
      "\n",
      "Ticket: \"The VPN disconnects every time I upload files.\"   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\":\n",
      "------------------------------------------------------------\n",
      "🔹 Ticket:\n",
      "Wi-Fi drops every morning at 9 AM in the office.\n",
      "✅ Model Output:\n",
      "Classify the following IT ticket and return output in JSON format with keys:\n",
      "'ticket', 'Department', 'Priority'.\n",
      "\n",
      "Ticket: \"Wi-Fi drops every morning at 9 AM in the office.\"   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\": \"Returns and Exchanges\", \"priority\": \"medium\"}   {\"department\":\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# initialize pipeline with your model\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    tokenizer=MODEL_ID,\n",
    "    device=0 if torch.cuda.is_available() else -1  # use GPU if available\n",
    ")\n",
    "\n",
    "# example ticket list\n",
    "sample_tickets = [\n",
    "    \"My laptop is not turning on after the latest update.\",\n",
    "    \"The VPN disconnects every time I upload files.\",\n",
    "    \"Wi-Fi drops every morning at 9 AM in the office.\"\n",
    "]\n",
    "\n",
    "for ticket in sample_tickets:\n",
    "    prompt = (\n",
    "        \"Classify the following IT ticket and return output in JSON format with keys:\\n\"\n",
    "        \"'ticket', 'Department', 'Priority'.\\n\\n\"\n",
    "        f\"Ticket: \\\"{ticket}\\\"\"\n",
    "    )\n",
    "\n",
    "    # include generation settings\n",
    "    output = pipe(prompt, max_new_tokens=128, do_sample=False)\n",
    "    text = output[0][\"generated_text\"].strip()\n",
    "\n",
    "    print(\"🔹 Ticket:\")\n",
    "    print(ticket)\n",
    "    print(\"✅ Model Output:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
