{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parth\\OneDrive\\Desktop\\one\\RouteIQ\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: gpt2\n",
      "Tokenizer loaded successfully.\n",
      "Loading model from: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Saving model and tokenizer to: ./gpt2-it-ticket-classifier\n",
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# First, make sure you have the transformers library installed:\n",
    "# pip install transformers accelerate torch # or tensorflow depending on your backend\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    " \n",
    "PRETRAINED_MODEL_NAME = \"gpt2\"  \n",
    "print(f\"Loading tokenizer from: {PRETRAINED_MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# --- 3. Load the model ---\n",
    " \n",
    "print(f\"Loading model from: {PRETRAINED_MODEL_NAME}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "print(\"Model loaded successfully.\")\n",
    " \n",
    "SAVE_PATH = \"./gpt2-it-ticket-classifier\" # Choose a meaningful local path\n",
    "\n",
    "print(f\"Saving model and tokenizer to: {SAVE_PATH}\")\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [05:18<00:00, 1.56MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/parth1609/gpt2-it-ticket-classifier/commit/e37987b6e8b86e9f730bce9896594ffbb302cb43', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e37987b6e8b86e9f730bce9896594ffbb302cb43', pr_url=None, repo_url=RepoUrl('https://huggingface.co/parth1609/gpt2-it-ticket-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='parth1609/gpt2-it-ticket-classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"./gpt2-it-ticket-classifier\",\n",
    "    repo_id=\"parth1609/gpt2-it-ticket-classifier\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Your model ID on Hugging Face Hub\n",
    "MODEL_ID = \"parth1609/gpt2-it-ticket-classifier\"\n",
    "\n",
    "# Your Hugging Face API token\n",
    "# It's highly recommended to load this from an environment variable for security\n",
    "# Make sure you've set HF_TOKEN=\"hf_YOUR_TOKEN_HERE\" in your environment\n",
    "# API_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "API_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not API_TOKEN:\n",
    "    raise ValueError(\"Hugging Face API token not found. Please set the HF_TOKEN environment variable.\")\n",
    "\n",
    "# The Inference API URL for your model\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\" # Specify content type for JSON payload\n",
    "}\n",
    "\n",
    "# --- Function to send a query to the API ---\n",
    "def query(payload):\n",
    "    \"\"\"\n",
    "    Sends a query (input text) to the Hugging Face Inference API.\n",
    "\n",
    "    Args:\n",
    "        payload (dict): A dictionary containing the input data for the model.\n",
    "                        For text models, it's typically {\"inputs\": \"Your input text\"}.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from the API, containing the model's predictions.\n",
    "    \"\"\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = [\n",
    "    \"My laptop is not turning on after the latest update.\",\n",
    "    \"I need help resetting my VPN password, it keeps failing.\",\n",
    "    \"The software update caused my computer to crash.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# include generation settings\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# output = pipe(prompt, max_new_tokens=512, do_sample=False)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# text = output[0][\"generated_text\"].strip()\u001b[39;00m\n\u001b[32m     42\u001b[39m out = pipe(prompt, max_new_tokens=\u001b[32m60\u001b[39m)[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# or eval(out) if itâ€™s valid Python dict\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# print(out)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ”¹ Ticket:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# initialize pipeline with your model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    tokenizer=MODEL_ID,\n",
    "    device=0 \n",
    ")\n",
    "\n",
    "# example ticket list\n",
    "sample_tickets = [\n",
    "    \"My laptop is not turning on after the latest update.\",\n",
    "    \"The VPN disconnects every time I upload files.\",\n",
    "    \"Wi-Fi drops every morning at 9 AM in the office.\"\n",
    "]\n",
    "\n",
    "for ticket in sample_tickets:\n",
    "    prompt = (\n",
    "        \"\"\"<|im_start|>system\n",
    "    You are an IT support specialist. Given a ticket description, you will **only** output valid JSON **with no additional text**, following this exact schema:\n",
    "    ```json\n",
    "    {\n",
    "    \"Department\": \"IT\",         // one of: IT, Customer Service, Sales, Billing\n",
    "    \"Priority\": \"High\"          // one of: High, Medium, Low\n",
    "}\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \"Classify this support ticket into Department and Priority:\\n\\n\"\n",
    "        f'\"\"\\n{ticket}\\n\"\"\\n'\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    # include generation settings\n",
    "    # output = pipe(prompt, max_new_tokens=512, do_sample=False)\n",
    "    # text = output[0][\"generated_text\"].strip()\n",
    "\n",
    "    out = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
    "    print(json.loads(out))  # or eval(out) if itâ€™s valid Python dict\n",
    "    # print(out)\n",
    "\n",
    "    print(\"ðŸ”¹ Ticket:\")\n",
    "    print(ticket)\n",
    "    print(\"âœ… Model Output:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
